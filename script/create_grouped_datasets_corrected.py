#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import shutil
import argparse
from collections import defaultdict

def create_grouped_datasets_corrected(original_dataset_dir, output_base_dir):
    """
    æ ¹æ®æ¨¡æ€å®Œæ•´æ€§åˆ›å»ºåˆ†ç»„æ•°æ®é›†ï¼ˆæ­£ç¡®ç‰ˆï¼‰
    
    å‚æ•°:
        original_dataset_dir (str): åŸå§‹æ•°æ®é›†ç›®å½•
        output_base_dir (str): è¾“å‡ºåŸºç¡€ç›®å½•
        
    è¿”å›:
        dict: åˆ†ç»„ä¿¡æ¯
    """
    print("=" * 60)
    print("åˆ›å»ºåˆ†ç»„æ•°æ®é›†ï¼ˆæ­£ç¡®ç‰ˆï¼‰")
    print("=" * 60)
    
    # æ£€æŸ¥åŸå§‹æ•°æ®é›†
    if not os.path.exists(original_dataset_dir):
        raise FileNotFoundError(f"åŸå§‹æ•°æ®é›†ç›®å½• {original_dataset_dir} ä¸å­˜åœ¨")
    
    dataset_json_path = os.path.join(original_dataset_dir, 'dataset.json')
    if not os.path.exists(dataset_json_path):
        raise FileNotFoundError("åŸå§‹æ•°æ®é›†ä¸­çš„ dataset.json æ–‡ä»¶ä¸å­˜åœ¨")
    
    # è¯»å–åŸå§‹æ•°æ®é›†ä¿¡æ¯
    with open(dataset_json_path, 'r', encoding='utf-8') as f:
        original_dataset = json.load(f)
    
    print(f"ğŸ“Š åŸå§‹æ•°æ®é›†åŒ…å« {original_dataset['numTraining']} ä¸ªè®­ç»ƒç—…ä¾‹")
    
    # åˆ†ææ¯ä¸ªç—…ä¾‹çš„æ¨¡æ€å®Œæ•´æ€§
    complete_cases = []  # åŒ…å«æ‰€æœ‰æ¨¡æ€çš„ç—…ä¾‹
    missing_cases = []   # ç¼ºå¤±gaoqing-T2æ¨¡æ€çš„ç—…ä¾‹
    
    images_dir = os.path.join(original_dataset_dir, 'imagesTr')
    labels_dir = os.path.join(original_dataset_dir, 'labelsTr')
    
    if not os.path.exists(images_dir):
        raise FileNotFoundError(f"åŸå§‹æ•°æ®é›† imagesTr ç›®å½•ä¸å­˜åœ¨: {images_dir}")
    
    if not os.path.exists(labels_dir):
        raise FileNotFoundError(f"åŸå§‹æ•°æ®é›† labelsTr ç›®å½•ä¸å­˜åœ¨: {labels_dir}")
    
    # æ£€æŸ¥æ¯ä¸ªç—…ä¾‹
    print("ğŸ” åˆ†ææ¯ä¸ªç—…ä¾‹çš„æ¨¡æ€å®Œæ•´æ€§...")
    case_analysis = []
    
    for i, case in enumerate(original_dataset['training']):
        image_base = case['image']
        label_path = case['label']
        
        if image_base.startswith('./'):
            image_base = image_base[2:]  # ç§»é™¤ "./" å‰ç¼€
        
        # æ£€æŸ¥æ¨¡æ€æ–‡ä»¶æ•°é‡ (0000åˆ°0004å…±5ä¸ªæ¨¡æ€)
        modality_files = []
        for mod_idx in range(5):
            modality_file = f"{os.path.basename(image_base)}_{mod_idx:04d}.nii.gz"
            full_image_path = os.path.join(images_dir, modality_file)
            
            if os.path.exists(full_image_path):
                modality_files.append(modality_file)
        
        case_info = {
            'case': case,
            'modality_files': modality_files,
            'count': len(modality_files)
        }
        case_analysis.append(case_info)
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªæ¡ˆä¾‹ä½œä¸ºç¤ºä¾‹
        if i < 5:
            print(f"   æ¡ˆä¾‹: {os.path.basename(image_base)} - {len(modality_files)} ä¸ªæ¨¡æ€")
    
    # åˆ†ç±»ç—…ä¾‹
    for case_info in case_analysis:
        if case_info['count'] == 5:
            complete_cases.append(case_info)
        elif case_info['count'] == 4:
            missing_cases.append(case_info)
    
    print(f"\nâœ… åˆ†æå®Œæˆ:")
    print(f"   å®Œæ•´æ¨¡æ€ç—…ä¾‹æ•° (5ä¸ªæ¨¡æ€): {len(complete_cases)}")
    print(f"   ç¼ºå¤±æ¨¡æ€ç—…ä¾‹æ•° (4ä¸ªæ¨¡æ€): {len(missing_cases)}")
    print(f"   å¼‚å¸¸ç—…ä¾‹æ•° (å…¶ä»–æ¨¡æ€æ•°): {len(case_analysis) - len(complete_cases) - len(missing_cases)}")
    
    # åˆ›å»ºå®Œæ•´æ¨¡æ€æ•°æ®é›† (Group A)
    group_a_dir = os.path.join(output_base_dir, 'Dataset999_ProstateBPHPCA_GroupA')
    create_group_dataset_corrected(group_a_dir, original_dataset, complete_cases, "å®Œæ•´æ¨¡æ€ç»„")
    
    # åˆ›å»ºç¼ºå¤±æ¨¡æ€æ•°æ®é›† (Group B)
    group_b_dir = os.path.join(output_base_dir, 'Dataset002_ProstateBPHPCA_GroupB')
    create_group_dataset_corrected(group_b_dir, original_dataset, missing_cases, "ç¼ºå¤±æ¨¡æ€ç»„", 
                                  exclude_modality_index=4)  # gaoqing-T2æ˜¯ç¬¬5ä¸ªæ¨¡æ€(ç´¢å¼•4)
    
    # ç”Ÿæˆåˆ†ç»„ä¿¡æ¯æŠ¥å‘Š
    grouping_info = {
        'group_a': {
            'name': 'å®Œæ•´æ¨¡æ€ç»„',
            'dataset_id': 1,
            'dataset_name': 'Dataset999_ProstateBPHPCA_GroupA',
            'cases_count': len(complete_cases),
            'modalities': ['ADC', 'DWI', 'T2_fs', 'T2_not_fs', 'gaoqing_T2']
        },
        'group_b': {
            'name': 'ç¼ºå¤±æ¨¡æ€ç»„',
            'dataset_id': 2,
            'dataset_name': 'Dataset002_ProstateBPHPCA_GroupB',
            'cases_count': len(missing_cases),
            'modalities': ['ADC', 'DWI', 'T2_fs', 'T2_not_fs']
        }
    }
    
    # ä¿å­˜åˆ†ç»„ä¿¡æ¯
    grouping_info_path = os.path.join(output_base_dir, 'grouping_info.json')
    with open(grouping_info_path, 'w', encoding='utf-8') as f:
        json.dump(grouping_info, f, indent=4, ensure_ascii=False)
    
    print(f"\nğŸ’¾ åˆ†ç»„ä¿¡æ¯å·²ä¿å­˜åˆ°: {grouping_info_path}")
    
    # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
    print("\n" + "=" * 60)
    print("ä½¿ç”¨è¯´æ˜")
    print("=" * 60)
    print("è®­ç»ƒå®Œæ•´æ¨¡æ€ç»„æ¨¡å‹:")
    print("  nnUNetv2_plan_and_preprocess -d 1 --verify_dataset_integrity")
    print("  nnUNetv2_train 1 3d_fullres 0")
    print("\nè®­ç»ƒç¼ºå¤±æ¨¡æ€ç»„æ¨¡å‹:")
    print("  nnUNetv2_plan_and_preprocess -d 2 --verify_dataset_integrity")
    print("  nnUNetv2_train 2 3d_fullres 0")
    print("\nè®¾ç½®ç¯å¢ƒå˜é‡:")
    print("  export nnUNet_raw='nnUNet_raw_data_base'")
    print("  export nnUNet_preprocessed='nnUNet_preprocessed'")
    print("  export nnUNet_results='nnUNet_trained_models'")
    
    return grouping_info

def create_group_dataset_corrected(dataset_dir, original_dataset, cases_with_files, group_name, exclude_modality_index=None):
    """
    åˆ›å»ºåˆ†ç»„æ•°æ®é›†ï¼ˆæ­£ç¡®ç‰ˆï¼‰
    
    å‚æ•°:
        dataset_dir (str): æ•°æ®é›†ç›®å½•
        original_dataset (dict): åŸå§‹æ•°æ®é›†ä¿¡æ¯
        cases_with_files (list): è¯¥ç»„çš„ç—…ä¾‹åˆ—è¡¨åŠæ–‡ä»¶åˆ—è¡¨
        group_name (str): ç»„åç§°
        exclude_modality_index (int): è¦æ’é™¤çš„æ¨¡æ€ç´¢å¼•
    """
    print(f"\nğŸ“ åˆ›å»º {group_name}: {dataset_dir}")
    
    # åˆ›å»ºç›®å½•ç»“æ„
    images_tr_dir = os.path.join(dataset_dir, 'imagesTr')
    labels_tr_dir = os.path.join(dataset_dir, 'labelsTr')
    os.makedirs(images_tr_dir, exist_ok=True)
    os.makedirs(labels_tr_dir, exist_ok=True)
    
    print(f"  ğŸ“ å¤åˆ¶ {len(cases_with_files)} ä¸ªç—…ä¾‹çš„æ–‡ä»¶...")
    
    # å¤åˆ¶å›¾åƒå’Œæ ‡ç­¾æ–‡ä»¶
    original_images_dir = os.path.join(os.path.dirname(dataset_dir), 
                                      'Dataset999_ProstateBPHPCA',
                                      'imagesTr')
    original_labels_dir = os.path.join(os.path.dirname(dataset_dir),
                                      'Dataset999_ProstateBPHPCA',
                                      'labelsTr')
    
    copied_cases = []
    for i, case_info in enumerate(cases_with_files):
        case = case_info['case']
        modality_files = case_info['modality_files']
        
        image_base = case['image']
        label_path = case['label']
        
        if image_base.startswith('./'):
            image_base = image_base[2:]
        
        if label_path.startswith('./'):
            label_path = label_path[2:]
        
        # å¤åˆ¶æ¨¡æ€æ–‡ä»¶
        for modality_file in modality_files:
            src_path = os.path.join(original_images_dir, modality_file)
            dst_path = os.path.join(images_tr_dir, modality_file)
            
            if os.path.exists(src_path):
                shutil.copy2(src_path, dst_path)
            else:
                print(f"     è­¦å‘Š: æºæ–‡ä»¶ä¸å­˜åœ¨ {src_path}")
        
        # å¤åˆ¶æ ‡ç­¾æ–‡ä»¶
        src_label_path = os.path.join(original_labels_dir, os.path.basename(label_path))
        dst_label_path = os.path.join(labels_tr_dir, os.path.basename(label_path))
        
        if os.path.exists(src_label_path):
            shutil.copy2(src_label_path, dst_label_path)
            copied_cases.append(case)
        else:
            print(f"     è­¦å‘Š: æ ‡ç­¾æ–‡ä»¶ä¸å­˜åœ¨ {src_label_path}")
        
        if (i + 1) % 50 == 0:
            print(f"     å·²å¤„ç† {i + 1} ä¸ªç—…ä¾‹...")
    
    print(f"  âœ… æˆåŠŸå¤åˆ¶ {len(copied_cases)} ä¸ªç—…ä¾‹çš„æ–‡ä»¶")
    
    # åˆ›å»ºæ–°çš„dataset.json
    if exclude_modality_index is not None:
        # ä¸ºç¼ºå¤±æ¨¡æ€ç»„åˆ›å»º4æ¨¡æ€é…ç½®
        new_channel_names = {}
        original_channels = original_dataset['channel_names']
        channel_idx = 0
        for i in range(5):  # åŸå§‹5ä¸ªæ¨¡æ€
            if i != exclude_modality_index:
                modality_name = original_channels.get(str(i), f"modality_{i:04d}")
                new_channel_names[str(channel_idx)] = modality_name
                channel_idx += 1
    else:
        # ä¸ºå®Œæ•´æ¨¡æ€ç»„ä¿ç•™æ‰€æœ‰æ¨¡æ€
        new_channel_names = original_dataset['channel_names']
    
    new_dataset = {
        "name": f"ProstateMultiModal_BPH_PCA_{group_name}",
        "description": f"Prostate segmentation with BPH and PCA cases - {group_name}",
        "reference": "Your Institution",
        "licence": "CC-BY-NC-SA 4.0",
        "release": "1.0",
        "channel_names": new_channel_names,
        "labels": original_dataset['labels'],
        "numTraining": len(copied_cases),
        "numTest": 0,
        "file_ending": original_dataset['file_ending'],
        "training": copied_cases,
        "test": []
    }
    
    dataset_json_path = os.path.join(dataset_dir, 'dataset.json')
    with open(dataset_json_path, 'w', encoding='utf-8') as f:
        json.dump(new_dataset, f, indent=4, ensure_ascii=False)
    
    print(f"  âœ… {group_name} æ•°æ®é›†åˆ›å»ºå®Œæˆï¼ŒåŒ…å« {len(copied_cases)} ä¸ªç—…ä¾‹")

def main():
    parser = argparse.ArgumentParser(description='åˆ›å»ºåˆ†ç»„æ•°æ®é›†ä»¥å¤„ç†æ¨¡æ€ç¼ºå¤±é—®é¢˜ï¼ˆæ­£ç¡®ç‰ˆï¼‰')
    parser.add_argument('--original_dataset_dir', required=True, 
                       help='åŸå§‹æ•°æ®é›†ç›®å½•')
    parser.add_argument('--output_base_dir', default='nnUNet_raw_data_base',
                       help='è¾“å‡ºåŸºç¡€ç›®å½• (é»˜è®¤: nnUNet_raw_data_base)')
    
    args = parser.parse_args()
    
    try:
        grouping_info = create_grouped_datasets_corrected(args.original_dataset_dir, args.output_base_dir)
        print(f"\nğŸ‰ åˆ†ç»„æ•°æ®é›†åˆ›å»ºæˆåŠŸ!")
        print(f"   å®Œæ•´æ¨¡æ€ç»„: {grouping_info['group_a']['cases_count']} ä¸ªç—…ä¾‹")
        print(f"   ç¼ºå¤±æ¨¡æ€ç»„: {grouping_info['group_b']['cases_count']} ä¸ªç—…ä¾‹")
        return 0
    except Exception as e:
        print(f"âŒ åˆ›å»ºåˆ†ç»„æ•°æ®é›†æ—¶å‡ºç°é”™è¯¯: {e}")
        return 1

if __name__ == "__main__":
    exit(main())