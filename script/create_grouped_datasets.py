#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import shutil
import argparse
from collections import defaultdict

def create_grouped_datasets(original_dataset_dir, output_base_dir):
    """
    æ ¹æ®æ¨¡æ€å®Œæ•´æ€§åˆ›å»ºåˆ†ç»„æ•°æ®é›†
    
    å‚æ•°:
        original_dataset_dir (str): åŸå§‹æ•°æ®é›†ç›®å½•
        output_base_dir (str): è¾“å‡ºåŸºç¡€ç›®å½•
        
    è¿”å›:
        dict: åˆ†ç»„ä¿¡æ¯
    """
    print("=" * 60)
    print("åˆ›å»ºåˆ†ç»„æ•°æ®é›†")
    print("=" * 60)
    
    # æ£€æŸ¥åŸå§‹æ•°æ®é›†
    if not os.path.exists(original_dataset_dir):
        raise FileNotFoundError(f"åŸå§‹æ•°æ®é›†ç›®å½• {original_dataset_dir} ä¸å­˜åœ¨")
    
    dataset_json_path = os.path.join(original_dataset_dir, 'dataset.json')
    if not os.path.exists(dataset_json_path):
        raise FileNotFoundError("åŸå§‹æ•°æ®é›†ä¸­çš„ dataset.json æ–‡ä»¶ä¸å­˜åœ¨")
    
    # è¯»å–åŸå§‹æ•°æ®é›†ä¿¡æ¯
    with open(dataset_json_path, 'r', encoding='utf-8') as f:
        original_dataset = json.load(f)
    
    print(f"ğŸ“Š åŸå§‹æ•°æ®é›†åŒ…å« {original_dataset['numTraining']} ä¸ªè®­ç»ƒç—…ä¾‹")
    
    # åˆ†ææ¯ä¸ªç—…ä¾‹çš„æ¨¡æ€å®Œæ•´æ€§
    complete_cases = []  # åŒ…å«æ‰€æœ‰æ¨¡æ€çš„ç—…ä¾‹
    missing_cases = []   # ç¼ºå¤±gaoqing-T2æ¨¡æ€çš„ç—…ä¾‹
    
    images_dir = os.path.join(original_dataset_dir, 'imagesTr')
    labels_dir = os.path.join(original_dataset_dir, 'labelsTr')
    
    # æ£€æŸ¥æ¯ä¸ªç—…ä¾‹
    for case in original_dataset['training']:
        image_base = case['image']
        if image_base.startswith('./'):
            image_base = image_base[2:]
        
        # æ£€æŸ¥æ¨¡æ€æ–‡ä»¶æ•°é‡
        modality_count = 0
        mod_idx = 0
        while True:
            modality_file = f"{image_base}_{mod_idx:04d}.nii.gz"
            full_image_path = os.path.join(original_dataset_dir, modality_file)
            
            if os.path.exists(full_image_path):
                modality_count += 1
                mod_idx += 1
            else:
                break
        
        # æ ¹æ®æ¨¡æ€æ•°é‡åˆ†ç±»
        if modality_count == 5:
            complete_cases.append(case)
        elif modality_count == 4:
            missing_cases.append(case)
        else:
            print(f"  âš ï¸  æ¡ˆä¾‹ {image_base} æœ‰å¼‚å¸¸æ¨¡æ€æ•°é‡: {modality_count}")
    
    print(f"âœ… å®Œæ•´æ¨¡æ€ç—…ä¾‹æ•°: {len(complete_cases)}")
    print(f"âœ… ç¼ºå¤±æ¨¡æ€ç—…ä¾‹æ•°: {len(missing_cases)}")
    
    # åˆ›å»ºå®Œæ•´æ¨¡æ€æ•°æ®é›† (Group A)
    group_a_dir = os.path.join(output_base_dir, 'Dataset001_ProstateBPHPCA_GroupA')
    create_group_dataset(group_a_dir, original_dataset, complete_cases, "å®Œæ•´æ¨¡æ€ç»„")
    
    # åˆ›å»ºç¼ºå¤±æ¨¡æ€æ•°æ®é›† (Group B)
    group_b_dir = os.path.join(output_base_dir, 'Dataset002_ProstateBPHPCA_GroupB')
    create_group_dataset(group_b_dir, original_dataset, missing_cases, "ç¼ºå¤±æ¨¡æ€ç»„", 
                        exclude_modality='gaoqing_T2')
    
    # ç”Ÿæˆåˆ†ç»„ä¿¡æ¯æŠ¥å‘Š
    grouping_info = {
        'group_a': {
            'name': 'å®Œæ•´æ¨¡æ€ç»„',
            'dataset_id': 1,
            'dataset_name': 'Dataset001_ProstateBPHPCA_GroupA',
            'cases_count': len(complete_cases),
            'modalities': ['ADC', 'DWI', 'T2_fs', 'T2_not_fs', 'gaoqing_T2']
        },
        'group_b': {
            'name': 'ç¼ºå¤±æ¨¡æ€ç»„',
            'dataset_id': 2,
            'dataset_name': 'Dataset002_ProstateBPHPCA_GroupB',
            'cases_count': len(missing_cases),
            'modalities': ['ADC', 'DWI', 'T2_fs', 'T2_not_fs']
        }
    }
    
    # ä¿å­˜åˆ†ç»„ä¿¡æ¯
    grouping_info_path = os.path.join(output_base_dir, 'grouping_info.json')
    with open(grouping_info_path, 'w', encoding='utf-8') as f:
        json.dump(grouping_info, f, indent=4, ensure_ascii=False)
    
    print(f"\nğŸ’¾ åˆ†ç»„ä¿¡æ¯å·²ä¿å­˜åˆ°: {grouping_info_path}")
    
    # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
    print("\n" + "=" * 60)
    print("ä½¿ç”¨è¯´æ˜")
    print("=" * 60)
    print("è®­ç»ƒå®Œæ•´æ¨¡æ€ç»„æ¨¡å‹:")
    print("  nnUNetv2_train 1 3d_fullres 0")
    print("\nè®­ç»ƒç¼ºå¤±æ¨¡æ€ç»„æ¨¡å‹:")
    print("  nnUNetv2_train 2 3d_fullres 0")
    print("\nè®¾ç½®ç¯å¢ƒå˜é‡:")
    print("  export nnUNet_raw='nnUNet_raw_data_base'")
    print("  export nnUNet_preprocessed='nnUNet_preprocessed'")
    print("  export nnUNet_results='nnUNet_trained_models'")
    
    return grouping_info

def create_group_dataset(dataset_dir, original_dataset, cases, group_name, exclude_modality=None):
    """
    åˆ›å»ºåˆ†ç»„æ•°æ®é›†
    
    å‚æ•°:
        dataset_dir (str): æ•°æ®é›†ç›®å½•
        original_dataset (dict): åŸå§‹æ•°æ®é›†ä¿¡æ¯
        cases (list): è¯¥ç»„çš„ç—…ä¾‹åˆ—è¡¨
        group_name (str): ç»„åç§°
        exclude_modality (str): è¦æ’é™¤çš„æ¨¡æ€åç§°
    """
    print(f"\nğŸ“ åˆ›å»º {group_name}: {dataset_dir}")
    
    # åˆ›å»ºç›®å½•ç»“æ„
    images_tr_dir = os.path.join(dataset_dir, 'imagesTr')
    labels_tr_dir = os.path.join(dataset_dir, 'labelsTr')
    os.makedirs(images_tr_dir, exist_ok=True)
    os.makedirs(labels_tr_dir, exist_ok=True)
    
    # å¤åˆ¶å›¾åƒæ–‡ä»¶
    print(f"  ğŸ“ å¤åˆ¶ {len(cases)} ä¸ªç—…ä¾‹çš„å›¾åƒæ–‡ä»¶...")
    for case in cases:
        image_base = case['image']
        if image_base.startswith('./'):
            image_base = image_base[2:]
        
        # å¤åˆ¶æ‰€æœ‰ç›¸å…³å›¾åƒæ–‡ä»¶
        mod_idx = 0
        while True:
            modality_file = f"{image_base}_{mod_idx:04d}.nii.gz"
            src_path = os.path.join(os.path.dirname(dataset_dir), 
                                   os.path.basename(os.path.dirname(dataset_dir)).replace('_GroupA', '').replace('_GroupB', ''),
                                   modality_file)
            
            if os.path.exists(src_path):
                dst_path = os.path.join(images_tr_dir, os.path.basename(modality_file))
                shutil.copy2(src_path, dst_path)
                mod_idx += 1
            else:
                break
    
    # å¤åˆ¶æ ‡ç­¾æ–‡ä»¶
    print(f"  ğŸ“ å¤åˆ¶ {len(cases)} ä¸ªç—…ä¾‹çš„æ ‡ç­¾æ–‡ä»¶...")
    for case in cases:
        label_path = case['label']
        if label_path.startswith('./'):
            label_path = label_path[2:]
        
        src_path = os.path.join(os.path.dirname(dataset_dir),
                               os.path.basename(os.path.dirname(dataset_dir)).replace('_GroupA', '').replace('_GroupB', ''),
                               label_path)
        dst_path = os.path.join(labels_tr_dir, os.path.basename(label_path))
        
        if os.path.exists(src_path):
            shutil.copy2(src_path, dst_path)
    
    # åˆ›å»ºæ–°çš„dataset.json
    if exclude_modality:
        # ä¸ºç¼ºå¤±æ¨¡æ€ç»„åˆ›å»º4æ¨¡æ€é…ç½®
        new_channel_names = {}
        original_channels = original_dataset['channel_names']
        channel_idx = 0
        for i in range(5):  # åŸå§‹5ä¸ªæ¨¡æ€
            modality_name = original_channels.get(str(i), f"modality_{i:04d}")
            if modality_name != exclude_modality:
                new_channel_names[str(channel_idx)] = modality_name
                channel_idx += 1
    else:
        # ä¸ºå®Œæ•´æ¨¡æ€ç»„ä¿ç•™æ‰€æœ‰æ¨¡æ€
        new_channel_names = original_dataset['channel_names']
    
    new_dataset = {
        "name": f"ProstateMultiModal_BPH_PCA_{group_name}",
        "description": f"Prostate segmentation with BPH and PCA cases - {group_name}",
        "reference": "Your Institution",
        "licence": "CC-BY-NC-SA 4.0",
        "release": "1.0",
        "channel_names": new_channel_names,
        "labels": original_dataset['labels'],
        "numTraining": len(cases),
        "numTest": 0,
        "file_ending": original_dataset['file_ending'],
        "training": cases,
        "test": []
    }
    
    dataset_json_path = os.path.join(dataset_dir, 'dataset.json')
    with open(dataset_json_path, 'w', encoding='utf-8') as f:
        json.dump(new_dataset, f, indent=4, ensure_ascii=False)
    
    print(f"  âœ… {group_name} æ•°æ®é›†åˆ›å»ºå®Œæˆ")

def main():
    parser = argparse.ArgumentParser(description='åˆ›å»ºåˆ†ç»„æ•°æ®é›†ä»¥å¤„ç†æ¨¡æ€ç¼ºå¤±é—®é¢˜')
    parser.add_argument('--original_dataset_dir', required=True, 
                       help='åŸå§‹æ•°æ®é›†ç›®å½•')
    parser.add_argument('--output_base_dir', default='nnUNet_raw_data_base',
                       help='è¾“å‡ºåŸºç¡€ç›®å½• (é»˜è®¤: nnUNet_raw_data_base)')
    
    args = parser.parse_args()
    
    try:
        grouping_info = create_grouped_datasets(args.original_dataset_dir, args.output_base_dir)
        print(f"\nğŸ‰ åˆ†ç»„æ•°æ®é›†åˆ›å»ºæˆåŠŸ!")
        print(f"   å®Œæ•´æ¨¡æ€ç»„: {grouping_info['group_a']['cases_count']} ä¸ªç—…ä¾‹")
        print(f"   ç¼ºå¤±æ¨¡æ€ç»„: {grouping_info['group_b']['cases_count']} ä¸ªç—…ä¾‹")
        return 0
    except Exception as e:
        print(f"âŒ åˆ›å»ºåˆ†ç»„æ•°æ®é›†æ—¶å‡ºç°é”™è¯¯: {e}")
        return 1

if __name__ == "__main__":
    exit(main())